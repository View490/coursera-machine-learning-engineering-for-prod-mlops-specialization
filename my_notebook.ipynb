{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Course 1 - Introduction to Machine Learning in Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview of ML Lifecycle and Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Beyond Notebook: Production ML isn't just a notebook, it's a hybrid.\n",
        "* Dual Expertise: ML teams need both ML and software engineering knowledges for building and deploying.\n",
        "* Product not Project: Not on creating a model, there are need for CI/CD model development.\n",
        "* Model Potential: To maximize the values of ML project, it nee to be how to deploy in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deployment Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Scenarios:\n",
        "* Camera take pictures of smartphones on a production line.\n",
        "* An API call sends the image to a prediction server (cloud or edge based)\n",
        "* The server analyzed images if phone is defective.\n",
        "\n",
        "Deployment Challenges:\n",
        "* More than ML Code: Need software to handle API calls, control decisions, and handle production specifics.\n",
        "* Edge vs Cloud: depend on factors ex. internet reliability, latency, and etc.\n",
        "* Real-World Surprises: Factory condition might differ from training environment, cuase issue called concept drift (e.g., dark lighting)\n",
        "* Bridging the Gap: Method to translate smoothly to production deployment by a little adaptating such as shadow deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Steps of ML Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Scope the Seas: define the goal of proeject, what is input and output and why we have to use ML model ?\n",
        "2. Gather Treasure: Stock up on valuable data, baseline, label, and organize it. Create a map that navigatable from the model.\n",
        "3. Train the Model: build, train, select, analyze errors of the models.\n",
        "4. Iterate and Explore: Back to data, model update, chcek project scoping.\n",
        "5. Pre-Deployment Check: makesure the model is suitable for deploying environment.\n",
        "6. Deploy: start deploy model in production by defined method.\n",
        "7. Monitoring and Matain: to avoid any huge error effect, monitoring such as logs and losses are great for avoid error.\n",
        "8. Refine: checks the CI/CD pipeline, concept drift, data distribution and wether or not the model still suit for business goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example - Speech Recognition\n",
        "\n",
        "**Scoping Stage**\n",
        "\n",
        "**Data Stage**\n",
        "\n",
        "**Modeling Stage**\n",
        "\n",
        "**Deployment Stage**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select and Train a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selecting and Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Analysis and Performance Auditing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Definition and Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Data and Establish Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Label and Organize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scoping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Laboratories***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LAB WEEK1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB1 - A Journal to Data***\n",
        "\n",
        "Objective and Methodology:\n",
        "* Experience the Data-Centric Model issues, diagnosis tools, and how to solve problems.\n",
        "* ***ISSUE :*** Data Imbalance, Accuracy_Score, Overfitting.\n",
        "* ***SOLUTIONS :*** Data Augmentaion, Balanced_Accuracy_Score.\n",
        "* ***CONSIDERATION :*** Same model with various kind of dataset.\n",
        "\n",
        ">Topics1: Imbalance Dataset\n",
        "* cause by data storage damages. The lossed data cause imbalance data issue.\n",
        "* imbalance data: is when some labels data has much more samples than other.\n",
        "* result: accuracy metric is misleading provides a false sense of model performance.\n",
        "* solve: use balanced_accuracy_score to see the see the average accuracy across labels.\n",
        "\n",
        ">Topics2: Training with Complete Daaset.\n",
        "* result: both accuracy and balanced_accuracy is close similar.\n",
        "* consideration: overfitting occur, from much difference in training and testing losses.\n",
        "\n",
        ">Topic3: Training with Data Augmentation\n",
        "* goal: increase data samples to solve imbalanece problem.\n",
        "* methods: img rotates, resizes, crops, filps.\n",
        "* ***warning :*** data augmentation must use in training dataset only.\n",
        "* result: better balanced_accuracy_score than imbalance dataset. Solved overfitting problem.\n",
        "\n",
        "Important Libraries:\n",
        "* tensorflow ImageDataGenerator: \\t create exmaple generator, and img augmentation.\n",
        "* tensorflow keras: create sequential model\n",
        "* skelarn metrics: model performance validation\n",
        "* os: file directory\n",
        "* pandas, numpy, matplotlib, seaborn: data visualizataion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LAB WEEK2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB2 - Data Labeling***\n",
        "\n",
        "Objectives and Methodologies:\n",
        "* Compare effects of ***Labeling Strategies*** on model performance.\n",
        "* Labeling Strategy including\n",
        "   * Randomly Generated Labels (performance lower bound).\n",
        "   * Automatic Generated Labels Based on Three Different Label Strategies.\n",
        "   * True Labels (performance upper bound).\n",
        "\n",
        "> Random Labeling.\n",
        "* normal distribution random number labels\n",
        "* ***RESULTS:*** accuracy = 50%\n",
        "\n",
        "> True Labels Values.\n",
        "* true labels expected to get highest performance.\n",
        "* ***RESULTS:*** accuracy = 91%\n",
        "\n",
        "> Automatic Labeling.\n",
        "\n",
        "* Defined Rules.(one labeling)\n",
        "   * defines dict of words that will be labeld as spam.\n",
        "   * not enough human define rules, comparing proportion with true labeled values.\n",
        "   * ***RESULTS:*** accuracy = 52%\n",
        "\n",
        "* Defined Better Rules.(two labeling)\n",
        "   * defines more dict of words that will be labeld as spam.\n",
        "   * defined dict of words that will be labels as NOT_spam.\n",
        "   * got higher datapoint (proportion of labeling and true-labeling.)\n",
        "   * ***RESULTS:*** accuracy = 70%\n",
        "\n",
        "* Defines More Rules.(included length rule)\n",
        "   * add length of words to rules of labeling.\n",
        "   * visualize the words legnth distribution to select parameter.\n",
        "   * ***RESULTS:*** accuracy = 86%\n",
        "\n",
        "Important Libraries:\n",
        "* sklearn metrics: accuracy score\n",
        "* sklearn naive_bayes: classifier\n",
        "* matplotlib, pandas: dataframe and labels visualization\n",
        "* sklearn extraction.text: vectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Course 2 - Machine Learning Data Lifecycle in Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collecting, Labeling and Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction to Machine Learning in ML Pipeline and MLEP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Collecting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Labeling Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering, Transformation and Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Transformation at Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Journey and Data Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction to ML Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evolving Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enterprise Data Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Labeling, Augmentation and Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing Different Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ***Laboratories***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LAB WEEK1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB3 - Tensorflow Data Validation***\n",
        "\n",
        "Objectives and Methodlogoies:\n",
        "* practice of TFDV from Tensorflow Extended (TFX)\n",
        "* TFDV provides insight of data analysis process including:\n",
        "   * baseline statistics.\n",
        "   * schema.\n",
        "   * serving data validation.\n",
        "   * find and fix data anomalies.\n",
        "\n",
        "> Generate Statisitcs and Schema.\n",
        "* Import Dataset.\n",
        "* train_df.\n",
        "   * add_extra_row, to create anomalies sample.\n",
        "   * generate train_stats.\n",
        "   * visualize train_stats.\n",
        "   * infer schema.\n",
        "   * display schema.\n",
        "* eval_df.\n",
        "   * generate eval_stats.\n",
        "   * visualize eval_stats args comparing to train_stats.\n",
        "   * filter out unnessesary values both train_df, eval_df.\n",
        "   * generate eval_stats and display both again.\n",
        "* validate_statistic by eval_stats and schema.\n",
        "* display it.\n",
        "\n",
        "> Revise Schema - to expand the anomalies thresholds.\n",
        "   * get_feature from schema.\n",
        "      * set new distribution_constrains.\n",
        "   * get_domain from schema.\n",
        "      * append domain values.\n",
        "   * set_domain numerical features.\n",
        "      * set feature range by schema_pb2.IntDomain\n",
        "* diplay validate_staistic, should be 'No anaomalies founded.\n",
        "\n",
        "> Examining Dataset Slices.\n",
        "* set slice_fn >>> slicing features.\n",
        "* pass schema and slice_fn to Statsoptions.\n",
        "* convert df to csv format (slicing support).\n",
        "* create sliced_stats from csv with args stats_option=StatOptions\n",
        "* use sliced_stats to generate and visualize_statistics\n",
        "* analyze the distribution of features on the selected slices.\n",
        "\n",
        "Important Libraries:\n",
        "* tensorflow: framework\n",
        "* tensorflow_data_validation: validate dataset.\n",
        "* tensorflow_metadata.proto.v0 : metadata and schema building.\n",
        "* tdfv utils slicing_util: get features for examining.\n",
        "* sklearn model_selection: data splitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB4 - Data Validation (Assignment)***\n",
        "1. Setup and Imports.\n",
        "    * directory, tf, tfdv, tfdv slicing, tf metadata.\n",
        "2. Load Dataset.\n",
        "    * pandas load dataset.\n",
        "    * splitting dataset to train_ eval, and serving dataset.\n",
        "3. Generate and Visualize Training Dataset.\n",
        "    * remove irrelevant features.\n",
        "    * Insttantiate StatsOptions with args feature_whitelist.\n",
        "    * train_stats from train_df, stats_options.\n",
        "    * visualzie train_stats.\n",
        "4. Infer Schema.\n",
        "    * create shema by infer_schema train_stats.\n",
        "    * display schema.\n",
        "5. Calcualte, Visualize and Fix Evaluation Anomalies.\n",
        "    * generate eval_stats.\n",
        "    * compare train_stats and eval_stats by visualize stats (set lhs_stats and rhs_stats).\n",
        "    * detect anomalies by display the validate_statistics of eval_stats and schema (define this function as cal_anomalies).\n",
        "    * fix evaluation anomalies schema by:\n",
        "        * get_domain features from schema.\n",
        "        * append new unique values.\n",
        "        * cal_anomalies, this should no anomalies found.\n",
        "6. Schema Environments.\n",
        "    * check anomalies in serving env by define new options.\n",
        "    * serving_stats from serving_df, options.\n",
        "    * cal_anomalies.\n",
        "    * relax the distribition of features.\n",
        "        *serving has anomalies <1%, then we set distribution_constraints.min_domain_mass=0.9 on anomalies features.\n",
        "        * cal_anomalies, still found anomalies on domain features.\n",
        "    * modify schema.\n",
        "        * many domain has same set of values [a,b,c,d] but the anomalies <1% has only [a] on train_stats and [a,b] on serving_stats that cause anaomalies. ['metfomin']\n",
        "        * set_domain for all features ['metfomin_a1'],['metfomin_2'], etc, that has same set of values to same domain.\n",
        "        * this will result in no anomalies founded since it detech for all values in same domain.\n",
        "        * print the domain value, will result in same set of values.\n",
        "    * anomalies detect with environments\n",
        "        * append default env to schema ('TRAINING', 'SERVING').\n",
        "        * exclude target_column (label) on SERVING env schema.\n",
        "        * cal_anomalies, should no anomalies found even no target col in serving_stats.\n",
        "7. Check for Data Dift and Skew.\n",
        "    * set skew_comparator and dift_comparator that expressed in L-infinity distance based on domain knowledges.\n",
        "    * create skew_drift_anomalies from validate_statistics by train_stats, schema, eval_stats, serving_stats (any stats we want).\n",
        "    * if not too much drift and skew in anay features we can ignore, but be consider because it can impact on model performance.\n",
        "8. Display Stats for Data Slices.\n",
        "    * get slice_fn by select feaures.\n",
        "    * generate stats for sliced dataset.\n",
        "    * display generated sliced data stats at specifified index (feature value).\n",
        "9. Freeze Schema.\n",
        "    * after schema was reviewed, then freeze schema.\n",
        "    * use tf io utils and TFDV write_schema_text()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LAB WEEK2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB5 - Simple Feature Engineering***\n",
        "\n",
        "Objectives and Methodologies:\n",
        "* use Tensorflow Transform to know under the hood of TFX Transform.\n",
        "* collect data, define metadata, create a preprocessing function, generate a constant graph with required transformations.\n",
        "\n",
        "> Define metadata.\n",
        "* define schema as  a dataset_Metadata object containing schema protobuf of dict mapping keys to features spec type.\n",
        "* display raw_data_metadata.\n",
        "\n",
        "> Create a Preprocessing Function (most important for tf.transformer).\n",
        "* preprocessin_fn\n",
        "    * extrat columns to local variables.\n",
        "    * data transformation using tft.\n",
        "    * return all values.\n",
        "\n",
        "> Generate a Constant Graph with Required Transformations.\n",
        "* TF Transformation use Apache Beam for scalability and flexibility. With pipe (|) operator.\n",
        "* define pipeline using Apache Beam context.\n",
        "* Analyze and transform dataset using preprocessing_fn, raw_data, raw_data_metadata.\n",
        "* unpack transform_dataset into transformed_data, transformed_metadata.\n",
        "* display result by ***pprint.pformat(var)***\n",
        "\n",
        "> \n",
        "\n",
        "Important Libraries:\n",
        "* Tensorflow: framework\n",
        "* tensorflow transformer: preprocessing\n",
        "* tensorflow metadata: create schema_utils "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB6 - Feature Engineering Pipeline***\n",
        "\n",
        "Objectives and Methodlogies:\n",
        "* Explore machine learning pipeline from Tensorflow Transform.\n",
        "* Feature Extraction built by Tensorflow Extension, including:\n",
        "    * ingest data from a base directory with `ExampleGen`.\n",
        "    * compute training data statistics with `StatisticsGen`.\n",
        "    * anomalies detect in evaluation data with `ExampleValidation`.\n",
        "    * Preprocess data into suitable features with `Transform`.\n",
        "\n",
        "> Setup, Define, and Visualize Dataset.\n",
        "* Import important libraries.\n",
        "* Import tfx.orchestration.\n",
        "* define: `pipeline_root`, `data_root`, `data_filepath`.\n",
        "* display dataset (head, matplotlib, etc)\n",
        "\n",
        "> Create and Run the Interactive Context.\n",
        "* `context` initial interactivecontent with local sqlite file by `pipeline_root`.\n",
        "\n",
        "> ExampleGen.\n",
        "* `CsvExampleGen <<< _data_root`.\n",
        "* instantiate `ExampleGen` with input csv dataset.\n",
        "    * this split data into training and evaluation sets (default: 2:1).\n",
        "    * convert each row into `tf.train.Example` suitable for TFX component.\n",
        "* `context.run(example_gen)`\n",
        "* get artifact object from example_gen output. {splitname, uri}\n",
        "    * if vertify the artifact uri, will show the data saved in gz zip.\n",
        "* To examine examples of the data saved in `TFRecord` format:\n",
        "    1. get list of files in directory (all compressed TFRecord files).\n",
        "    2. create `TFRecordDataset` to read these files.\n",
        "\n",
        "> StatisticsGen.\n",
        "* `StatisticsGen <<< ExampleGen`.\n",
        "* this `StatisticsGen` component use TFDV under the hood.\n",
        "* we can show statistics output by `context.show(...)`.\n",
        "* same as previous lab, statisticsGen record all statistics values.\n",
        "\n",
        "> SchemaGen.\n",
        "* `SchemaGen <<< StatisticsGen`\n",
        "\n",
        "> ExampleValidator.\n",
        "* `ExampleValidator <<< [StatisticsGen, SchemaGen]`\n",
        "\n",
        "> Transform.\n",
        "* perform feature engineering for both training and serving dataset.\n",
        "* These will run with under the hood by ApacheBeam.\n",
        "1. set constant module file. `data_constant.py`\n",
        "    * `%%writefile {data_constant.py}`.\n",
        "    * indicate `CATEGORICAL_FEATURES`<<< converted to indices.\n",
        "    * indicate `NUMERICAL_FEATURES` <<< continuous.\n",
        "    * indicate `BUCKET_FEATURES` <<< numerical that will be bucketized.\n",
        "    * indicate `FEATURE_BUCKET_COUNT` <<< dict of each bucket number.\n",
        "    * indicate `LABEL` that model will predict.\n",
        "2. set transform modeul file. `data_transform.py`.\n",
        "    * `%%writefile {data_transform.py}`.\n",
        "    * unpack all content from `data_constant.py`.\n",
        "    * preprocessing_fn:\n",
        "        - `NUMERICAL_FEATURE` scale_to_0_1.\n",
        "        - `BUCKET_FEATURE` bucketize as bucket count.\n",
        "        - `CATEGORICAL_FEATURE` convert into indices.\n",
        "        - `LABEL` convert label string into indices.\n",
        "3. Instantiate the Transform Component.\n",
        "    * `Transform <<< [ExampleGen, SchemaGen, data_transform.py]`.\n",
        "4. The Examine the output artificats:\n",
        "    * get uri from transformed_example\n",
        "    * get list of file in train_uri.\n",
        "    * create `TFRecordDataset` to read these files.\n",
        "    * use util function (own) get_record() to get samples of transform data.\n",
        "\n",
        "Important Libraries:\n",
        "* Tensorflow: framework.\n",
        "* tfx.components: all tfx for transformer pipeline.\n",
        "* tfx orchestration: interactive context.\n",
        "* google MessageToDict: Vocabulary Indication.\n",
        "* os: directory control.\n",
        "* pprint: Pretty Printer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB7 - Feature Selection***\n",
        "\n",
        "***Objectives and Methodlogies:***\n",
        "* Picking sets of features that are most relevant to target variable.\n",
        "* Reducing model complexity.\n",
        "* Breast Cancer Dataset.\n",
        "- ***Filter Methods:*** \n",
        "    - correlation with target variable.\n",
        "    - correlation with other features.\n",
        "    - unvariate Selection with scikit-learn.\n",
        "- ***Wrapper Methods:***\n",
        "    - recursive feature elimination.\n",
        "    - sequential feature selector.\n",
        "- ***Embedded Methos:***\n",
        "    - feature importances.\n",
        "    - L1 regularization.\n",
        "\n",
        "> Import Libraries and Load Dataset:\n",
        "* `sklearn` modules for feature selection and model evaluation.\n",
        "* `matplotlib, seaborn` for visualization.\n",
        "* load breast cancer data via pandas df.\n",
        "\n",
        "> Data Cleaning:\n",
        "* remove unwanted features,including \"unnamed col, id\"\n",
        "* integer encode. change string to int for binary-classifier.\n",
        "\n",
        "> Function for model trianing and evaluation.\n",
        "- input (X,Y) >>> sklearn standardscaler\n",
        "- RandomforestClassifier >>> train with (x_train, y_train) >>> sklearn evaluates metrics (x_test, y_test)\n",
        "- construct dataframe to display as \"all features\".\n",
        "\n",
        "> Correlation Matrix.\n",
        "* corr() see Pearson correlation score on each features.\n",
        "\n",
        "> Filter Method.\n",
        "* rank a given set of features from statistical methods.\n",
        "\n",
        "    > Correlation with Target Variable.\n",
        "    * strongness of features correlated with dianosis (target var).\n",
        "    * RESULT: higher accuracy and F1 score.\n",
        "\n",
        "    > Correlation with Other Features.\n",
        "    * use for remove features that highly correlated with other features, to remove redundant features for simpler model.\n",
        "    * RESULT: less feature inputs, faster training, same accuracy and F1-score as strong feature performance.\n",
        "\n",
        "    > Unvariate Selection with Scikit-Learn.\n",
        "    * ANOVA F-values to select top 20 features.\n",
        "    * RESULT: same accuracy and F1-score with lower feature count.\n",
        "\n",
        "> Wrapper Methods.\n",
        "* Methods to measure effectiveness of subset of features.\n",
        "    > SequentialFeatureSelector.\n",
        "    * can be used for adding or reducing features with a k-fold coress validation to see the effectiveness of features. Re\n",
        "    > Recursive Feature Elinmination.\n",
        "    * backward eliminatino but use feature importance scores to prune features.\n",
        "    * RESULT: slightly drop performance.\n",
        "\n",
        "> Embedded Methods.\n",
        "* use power of machine learning brnaches type such as randomforest classifier to see feature importances.\n",
        "    > Feature Importances.\n",
        "    * after `SelectFromModel` the model can reduce featurecount base on its importances.\n",
        "    * RESULT: less feature count, same accuracy and F1-score as RFE\n",
        "    > L1 Regularaization Feature Selector.\n",
        "    * Lasso Regularization itroduce a penalty term to loss function.\n",
        "    * `SelectFromModel(LinearSVC(C=1, penalty='l1'))`\n",
        "    * RESULT: less accuracy and F1-score\n",
        "\n",
        "***Summarize:***\n",
        "* if you focus on F1-score select `Strong Features, Subset Features, and F-Test`. On the other hands, if all scores are acceptable, then use the smallest set of features.\n",
        "\n",
        "***Important Libraries:***\n",
        "* Tensorflow: framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB8 - Feature Engineering (Assignment)***\n",
        "\n",
        "***Objectives and Methodlogies:***\n",
        "* ***TFX component*** to prepare features from Metro dataset.\n",
        "* ***InteractiveContext*** to run TFX components.\n",
        "* ***ExampleGen*** to split dataset.\n",
        "* ***StatsticsGen*** and ***SchemaGen***.\n",
        "* ***ExampleValidator***.\n",
        "* ***TFX Transformer*** component performed feature engineering.\n",
        "\n",
        "> Setup\n",
        "* Import and Define Paths.\n",
        "    - tfx omponents, tft_beam, google MessageToDict, tfx interactiveConetxt, tft metadata, \n",
        "    - tempfile, pprint\n",
        "    - _pipeline_root. _data_root, _data_filepath\n",
        "\n",
        "* Preview the Dataset.\n",
        "    - !head {_adta_filepath}\n",
        "\n",
        "* Create the InteractiveContext.\n",
        "    - InteractiveContext(pipeline_root)\n",
        "\n",
        "> Run TFX components interactively\n",
        "* ExampleGen\n",
        "    - `CsvExampleGen(_data_root)` <<< conntext.run()\n",
        "    - get artifact for training example and create ***TFRecordDataset*** to read these files.\n",
        "    - define function for get recorded data.\n",
        "\n",
        "* StatisticsGen\n",
        "    - `StatisticsGen(CsvExampleGen.outputs['examples'])` <<< context.run()\n",
        "    - context.show()\n",
        "\n",
        "* SchemaGen\n",
        "    - `SchemaGen(StatisticsGen.outputs['statistics'])` <<< context.run()\n",
        "    - context.show()\n",
        "\n",
        "* ExampleValdiator\n",
        "    - `ExampleValidator(Stats, Schema)` <<< context.run()\n",
        "    - context.show(['anomalies])\n",
        "\n",
        "* Transform\n",
        "    - %%writefile constants_module_file\n",
        "        + DENSE_FLOAT_FAETURE (z-score)\n",
        "        + BUCKET_FEATURE\n",
        "        + BUCKET_COUNT\n",
        "        + RANGE_FEATURE (0, 1)\n",
        "        + VOCAB_SITE\n",
        "        + OOV_SIZE\n",
        "        + VOCAB_FEATURE\n",
        "        + CATEGORICAL_FEATURE\n",
        "        + PREDICT_FEATURE (target)\n",
        "    - %%writefile transform_module_file\n",
        "        + unpack all contents from constant module.\n",
        "        + def preprocessing_fn:\n",
        "            - scale_to_z_score\n",
        "            - scale_to_0_1\n",
        "            - compute_and-apply_vocab\n",
        "            - bucketize\n",
        "            - do nothing for CATEGORICAL\n",
        "            - tf cast target to 1 and 0 if target more than mean\n",
        "    - test preprocessing_fn with `tft_beam`\n",
        "    - use MessageToDict to see if transformed_metadata.schema match SchemaGen\n",
        "    - Instantiate Transform Component.\n",
        "        + Transform(exampeGen, SchemaGen, transform_module_file)\n",
        "        + context.run()\n",
        "        + get graph_uri, train_uri, list files directory, and create `TFRecordDataset`\n",
        "\n",
        "***Important Libraries:***\n",
        "+ os: control directory.\n",
        "+ tensorflow: framework.\n",
        "+ tfx.components: Pipeline components.\n",
        "+ tft_beam: scalable transofrm pipeline.\n",
        "+ tfx.orchestration: InteractiveContext.\n",
        "+ tempfile: temp_dir for preprocessing pipeline testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LAB WEEK3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ***LAB9 - Walkthrough of ML Metadata***\n",
        "* API for tracking of any progress made in ML projects, especially when running TFX pipelines. Each components automatically records information to metadata stores.\n",
        "* It allows to retrieve information such as name of trianing splits, location of inferred schema.\n",
        "\n",
        "Objectives and Methodologies:\n",
        "* ML Metadata will be used directly for recording and retrieving metadata independent from TFX pipeline ( without TFX components).\n",
        "* TFDV for creating schema.\n",
        "\n",
        "> Import Libraries and Download daaset.\n",
        "* import libraries and version checking for {tf, tfdv}.\n",
        "* use urllib and zip to download zip and extract files.\n",
        "\n",
        "> Define ML Metadata's Storage Database.\n",
        "* Instantiate a connection config by `metadata_store_pb2`.\n",
        "* set parent database (inthis exp use fake_database).\n",
        "* setup Metadata store <<< connecting_config.\n",
        "\n",
        "> ArtifactType and ExecutionTypes - setting up .\n",
        "* create ArtifactType for input dataset by setting up name, and properties.\n",
        "* create ArtifactType for Schema by setting up name, and properties.\n",
        "* `put_artifact_type` register both artifacts to meatadata store.\n",
        "* create ExecutionType for TFDV by setting up name, and properties.\n",
        "* `put_schema_type` register execution type to metadata store.\n",
        "* ***RETURN*** - `data_artifact_type_id`, `schema_artifact_type_id`, `dv_execution_type_id`. \n",
        "\n",
        "> Artifact and Execution Unit - generating input .\n",
        "* declare input artifact of dataset including \"path, id, names, split_name, version\"\n",
        "* `put_artifacts` register to metadata store.\n",
        "* declare input Execution of Data Validation run including \"id, state\".\n",
        "* `put_executions` register to metadata store.\n",
        "* ***RETURN*** - `data_artifact_id`, `dv_execution_id`\n",
        "\n",
        "> Register input event.\n",
        "* This event define relationship between \"Artifacts and Executions\".\n",
        "* declare input event, by data_artifact_id, dv_execution_id, DECLARE_INPUT.\n",
        "* `put_events` into metadata store.\n",
        "\n",
        "> Run the TDFV component.\n",
        "* infer schema by passing train_data, train_stats into `tfdv.infer_schema`\n",
        "* write schema text file <<< schema_file.\n",
        "* ***RETURN*** - `schema_file`.\n",
        "\n",
        "> Artifact Unit - generate output.\n",
        "* declare output artifact of type schema_artifact including \"shema_file, schema_artifact_type_id, version, name\".\n",
        "* `put_artifact` register to metadata store by schema_artifact.\n",
        "* ***RETURN*** - `schema_artifact_id`\n",
        "\n",
        "> Register output event.\n",
        "* This event record output artifact of particular execution unit.\n",
        "* declare output event, by schema_artifact_id, dv_execution_id, DECLARE_OUTPUT.\n",
        "* `put_events` register to metadata store.\n",
        "\n",
        "> Update the execution unit.\n",
        "* mark state of dv_execution as COMPLETE.\n",
        "* `put_executions` register to metadata store by dv_execution.\n",
        "\n",
        "> Setting up Context Types.\n",
        "* Context Unit group to artifacts and execution units.\n",
        "* create ConntextType by \"name, and properties\".\n",
        "* `put_context_type` register to meatadata store.\n",
        "***RETURN*** - `expt_context_type`.\n",
        "\n",
        "> Generating a Context Unit\n",
        "* generate context by \"name, and propertise\" linked type_id to expt_conetext_type.\n",
        "* ***RETURN*** - `expt_context`.\n",
        "\n",
        "> Generate Attribution and association relationships.\n",
        "* generate Attricution with \"schema_artifact_id, and expt_context_id\".\n",
        "* generate Association with \"dv_execution_id, and expt_context_id\".\n",
        "* `put_attributions_and_associations` register to metadata store.\n",
        "* ***RETURN*** - `expt_attribution`, `expt_association`.\n",
        "\n",
        "> Retrieving Information from the Metadata Stores.\n",
        "* Now you can track everything without seeing the code by retrieving metadata store informations.\n",
        "* example usages:\n",
        "    + get_artifact_type\n",
        "    + invest elements in list of get_artifact_type\n",
        "    + get_events_by_artifact_ids\n",
        "    + get_events_by_execution_ids\n",
        "    + get artifacts_by_id\n",
        "\n",
        "Important Libraries:\n",
        "* ml_metadata: metadata storage, and proto(instantiate units and types).\n",
        "* tensorflow: framework.\n",
        "* tfdv: data validation.\n",
        "* urlfile, zipfile: files control and zipping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Course 3 - Machine Learning Modeling Pipelines in Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural Architecture Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters Tuning: Searching for Best Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AutoML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AutoML Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Resource Management Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantization and Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## High-Performance Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### High-Performance Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Analysis (TFMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Analysis Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Mode Analysis and Debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Continuous Evaluation and Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explainable AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Model Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Course 4 - Deploying Machine Learning Models in Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Serving Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction to Model Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Serving Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensorflow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Serving Patterns and Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Serving Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Online Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Inference Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Processing with ETL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Management and Delivery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML Experience Management and Workflow Automation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLOPs Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Management and Deployment Infrastructure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Monitoring and Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Monitoring and Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GDPR and Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
